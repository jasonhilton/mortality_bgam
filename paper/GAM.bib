Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Bryant2013,
author = {Bryant, John R. and Graham, Patrick J.},
doi = {10.1214/13-BA820},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bryant, Graham - 2013 - Bayesian demographic accounts Subnational population estimation using multiple data sources(2).pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Demography,Hierarchical bayesian model,MCMC,Official statistics,Population estimation},
number = {3},
pages = {591--622},
title = {{Bayesian demographic accounts: Subnational population estimation using multiple data sources}},
volume = {8},
year = {2013}
}
@article{Dowd2010,
abstract = {This study sets out a framework to evaluate the goodness of fit of stochastic mortality models and applies it to six different models estimated using English {\&} Welsh male mortality data over ages 64-89 and years 1961-2007. The methodology exploits the structure of each model to obtain various residual series that are predicted to be iid standard normal under the null hypothesis of model adequacy. Goodness of fit can then be assessed using conventional tests of the predictions of iid standard normality. The models considered are: Lee and Carter's (1992) one-factor model, a version of Renshaw and Haberman's (2006) extension of the Lee-Carter model to allow for a cohort-effect, the age-period-cohort model, which is a simplified version of the Renshaw-Haberman model, the 2006 Cairns-Blake-Dowd two-factor model and two generalized versions of the latter that allow for a cohort-effect. For the data set considered, there are some notable differences amongst the different models, but none of the models performs well in all tests and no model clearly dominates the others. {\textcopyright} 2010 Elsevier B.V.},
author = {Dowd, Kevin and Cairns, Andrew J G and Blake, David and Coughlan, Guy D. and Epstein, David and Khalaf-Allah, Marwa},
doi = {10.1016/j.insmatheco.2010.06.006},
file = {:C$\backslash$:/mendeleydump/1-s2.0-S0167668710000752-main.pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Goodness of fit,Mortality models,Standard normality},
number = {3},
pages = {255--265},
publisher = {Elsevier B.V.},
title = {{Evaluating the goodness of fit of stochastic mortality models}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2010.06.006},
volume = {47},
year = {2010}
}
@article{Lee1994,
abstract = {"This article presents and implements a new method for making stochastic population forecasts that provide consistent probability intervals. We blend mathematical demography and statistical time series methods to estimate stochastic models of fertility and mortality based on U.S. data back to 1900 and then use the theory of random-matrix products to forecast various demographic measures and their associated probability intervals to the year 2065. Our expected total population sizes agree quite closely with the Census medium projections, and our 95 percent probability intervals are close to the Census high and low scenarios. But Census intervals in 2065 for ages 65+ are nearly three times as broad as ours, and for 85+ are nearly twice as broad. In contrast, our intervals for the total dependency and youth dependency ratios are more than twice as broad as theirs, and our ratio for the elderly dependency ratio is 12 times as great as theirs. These items have major implications for policy, and these contrasting indications of uncertainty clearly show the limitations of the conventional scenario-based methods."},
author = {Lee, R D and Tuljapurkar, S},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Tuljapurkar - 1994 - Stochastic population forecasts for the United States beyond high, medium, and low(2).pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Age Factors,Americas,Demography,Developed Countries,Forecasting,Methods,North America,Population,Population Characteristics,Public Policy,Statistics as Topic,United States},
month = {dec},
number = {428},
pages = {1,175--89},
pmid = {12155397},
title = {{Stochastic population forecasts for the United States: beyond high, medium, and low.}},
url = {http://www.jstor.org/stable/10.2307/2290980},
volume = {89},
year = {1994}
}
@inproceedings{Palin2016,
address = {London},
author = {Palin, Jon},
booktitle = {International Mortality and Longevety Symposium},
file = {:C$\backslash$:/mendeleydump/C2 When is a Cohort not a Cohort.pdf:pdf},
publisher = {Institute and Faculty of Actuaries},
title = {{When is a cohort not a cohort ? Spurious parameters in stochastic longevity models}},
url = {https://www.actuaries.org.uk/documents/c2-when-cohort-not-cohort-spurious-parameters-stochastic-longevity-models},
year = {2016}
}
@article{Willets2004,
author = {Willets, R. C.},
doi = {10.1017/S1357321700002762},
file = {:C$\backslash$:/mendeleydump/sm20040426cohort.pdf:pdf},
issn = {1357-3217},
journal = {British Actuarial Journal},
keywords = {cohort effect,insights and explanations},
number = {04},
pages = {833--877},
title = {{The Cohort Effect: Insights and Explanations}},
volume = {10},
year = {2004}
}
@article{Cairns2009,
author = {Cairns, Andrew J G and Blake, David and Dowd, Kevin and Coughlan, D and Epstein, David and Ong, Alen},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cairns et al. - 2009 - A Quantitative Comparison of Stochastic Mortality Models Using Data from England and Wales and the United Stat(2).pdf:pdf},
issn = {1092-0277},
journal = {North American Actuarial Journal},
keywords = {and,change within european energy,coming,consolidation,deployment,directive has been the,energy in europe is,european commission,key recent driver of,phase of promoting renewable,regulation,s abstract the first,s single electricity market,sem,sustainable energy,the timetable of the,to an end},
pages = {1--35},
title = {{A Quantitative Comparison of Stochastic Mortality Models Using Data from England and Wales and the United States}},
volume = {13},
year = {2009}
}
@article{rhglm,
abstract = {The paper presents a reinterpretation of the model underpinning the
Lee-Carter methodology for forecasting mortality (and other vital)
rates. A parallel methodology based on generalized linear modelling
is introduced. The use of residual plots is proposed for both methods
to aid the assessment of the goodness of fit. The two methods are
compared in terms of structure and assumptions. They are then compared
through an analysis of the gender- and age-specific mortality rates
for England and Wales over the period 1950-M998 and through a consideration
of the forecasts generated by the two methods. The paper also compares
different approaches to the forecasting of life expectancy and considers
the effectiveness of the Coale-Guo method for extrapolating mortality
rates to the oldest ages.},
author = {Renshaw, Arthur and Haberman, Steven},
issn = {00359254},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {1},
pages = {pp. 119--137},
publisher = {Blackwell Publishing for the Royal Statistical Society},
title = {{Lee-Carter Mortality Forecasting: A Parallel Generalized Linear Modelling Approach for England and Wales Mortality Projections}},
url = {http://www.jstor.org/stable/3592636},
volume = {52},
year = {2003}
}
@article{Wood2016,
abstract = {The BUGS language offers a very flexible way of specifying complex statistical models for the purposes of Gibbs sampling, while its JAGS variant offers very convenient R integration via the rjags package. However, including smoothers in JAGS models can involve some quite tedious coding, especially for multivariate or adaptive smoothers. Further, if an additive smooth structure is required then some care is needed, in order to centre smooths appropriately, and to find appropriate starting values. R package mgcv implements a wide range of smoothers, all in a manner appropriate for inclusion in JAGS code, and automates centring and other smooth setup tasks. The purpose of this note is to describe an interface between mgcv and JAGS, based around an R function, `jagam', which takes a generalized additive model (GAM) as specified in mgcv and automatically generates the JAGS model code and data required for inference about the model via Gibbs sampling. Although the auto-generated JAGS code can be run as is, the expectation is that the user would wish to modify it in order to add complex stochastic model components readily specified in JAGS. A simple interface is also provided for visualisation and further inference about the estimated smooth components using standard mgcv functionality. The methods described here will be un-necessarily inefficient if all that is required is fully Bayesian inference about a standard GAM, rather than the full flexibility of JAGS. In that case the BayesX package would be more efficient.},
archivePrefix = {arXiv},
arxivId = {1602.02539},
author = {Wood, Simon N},
eprint = {1602.02539},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood - 2016 - Just Another Gibbs Additive Modeller Interfacing JAGS and mgcv(2).pdf:pdf},
journal = {arXiv},
keywords = {BUGS,JAGS,R,additive model,bugs,gen,generalized additive mixed model,jags,r,smooth,spline},
number = {1602.02539},
title = {{Just Another Gibbs Additive Modeller: Interfacing JAGS and mgcv}},
url = {http://arxiv.org/abs/1602.02539},
year = {2016}
}
@article{Gneiting2007,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G?=F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster tomake careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed.We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation(2).pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
number = {477},
pages = {359--378},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
volume = {102},
year = {2007}
}
@article{Wood2004,
author = {Wood, Simon N},
doi = {10.1198/016214504000000980},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood - 2004 - Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models(2).pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {1986,1990,a generalized additive model,gam,generalized additive mixed model,generalized cross-validation,glm,hastie and tibshirani,is a generalized linear,mccullagh,model,penalized quasi-likelihood,regularization,reml,ridge regression,smoothing spline analysis of,spline,stable computation,variance},
number = {467},
pages = {673--686},
title = {{Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000980},
volume = {99},
year = {2004}
}
@article{Gelman1992,
abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were contin- ued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normal- ity after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random- effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
author = {Gelman, Andrew and Rubin, Donald B},
file = {:C$\backslash$:/mendeleydump/gelmanrubin.pdf:pdf},
journal = {Statistical Science},
keywords = {algorithm,and phrases,bayesian inference,convergence of stochastic,ecm,em,gibbs sampler,importance sampling,metropolis,multiple imputation,processes,random-effects model,sir},
number = {4},
pages = {457--472},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
volume = {7},
year = {1992}
}
@article{Raftery2005,
abstract = {Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging (BMA), which is a standard method for combining predictive distributions from different sources. The BMA predictive probability density function (PDF) of any quantity of interest is a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The BMA weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The BMA PDF can be represented as an unweighted ensemble of any desired size, by simulating from the BMA predictive distribution. The BMA predictive var iance can be decomposed into two components, one corresponding to the between-forecast variability, and the second to the within-forecast variability. Predictive PDFs or intervals based solely on the ensemble spread incorporate the first component but not the second. Thus BMA provides a theoretical explanation of the tendency of ensembles to exhibit a spread-error correlation but yet be underdispersive. The method was applied to 48-h forecasts of surface te mperature in the Pacific Northwest in January-June 2000 using the University of Washington fifth-generation Pennsylvania State University-NCAR Mesoscale Model (MM5) ensemble. The predictive PDFs were much better calibrated than the raw ensemble, and the BMA forecasts were sharp in that 90{\%} BMA prediction intervals were 66{\%} shorter on average than those produced by sample climatology. As a by-product, BMA yields a deterministic point forecast, and this had root-mean-square errors 7{\%} lower than the best of the ensemble members and 8{\%} lower than the ensemble mean. Similar results were obtained for forecasts of sea level pressure. Simulation experiments show that BMA performs reasonably well when the underlying ensemble is calibrated, or even overdispersed. {\textcopyright} 2005 American Meteorological Society.},
author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
doi = {10.1175/MWR2906.1},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Forecast Ensembles(2).pdf:pdf},
isbn = {0027064415200493},
issn = {0027-0644},
journal = {Monthly Weather Review},
number = {5},
pages = {1155--1174},
title = {{Using Bayesian Model Averaging to Calibrate Forecast Ensembles}},
volume = {133},
year = {2005}
}
@techreport{Bijak2015a,
author = {Bijak, Jakub and Dodd, Erengul and Forster, Jonathon J. and Smith, Peter W. F.},
institution = {Office of National Statistics},
title = {{English Life Tables No. 17 Methodology}},
year = {2015}
}
@article{Bohk2015,
author = {Bohk, Christina and Rau, Roland and Cohen, Joel E},
doi = {10.4054/DemRes.2015.33.21},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bohk-Ewald, Rau, Cohen - 2015 - Taylor's power law in human mortality(2).pdf:pdf},
journal = {Demographic Research},
number = {21},
pages = {589--610},
title = {{Taylor's power law in human mortality}},
volume = {33},
year = {2015}
}
@article{Lewandowski2009,
abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276-294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177-2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
doi = {10.1016/j.jmva.2009.04.008},
file = {:C$\backslash$:/mendeleydump/1-s2.0-S0047259X09000876-main.pdf:pdf},
isbn = {0047-259X},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Correlation matrix,Dependence vines,Onion method,Partial correlation},
number = {9},
pages = {1989--2001},
publisher = {Elsevier Inc.},
title = {{Generating random correlation matrices based on vines and extended onion method}},
url = {http://dx.doi.org/10.1016/j.jmva.2009.04.008},
volume = {100},
year = {2009}
}
@misc{hmd,
annote = {Accessed on 26/07/11},
author = {{Human Mortality Database}},
institution = {University of California, Berkeley (USA), and Max Planck Institute for Demographic Research (Germany)},
title = {{Human Mortality Database}},
url = {http://www.mortality.org/cgi-bin/hmd},
year = {2016}
}
@article{Azose2016,
abstract = {We produce probabilistic projections of population for all countries based on probabilistic projections of fertility, mortality, and migration. We compare our projections to those from the United Nations' Probabilistic Population Projections, which uses similar methods for fertility and mortality but deterministic migration projections. We find that uncertainty in migration projection is a substantial contributor to uncertainty in population projections for many countries. Prediction intervals for the populations of Northern America and Europe are over 70{\%} wider, whereas prediction intervals for the populations of Africa, Asia, and the world as a whole are nearly unchanged. Out-of-sample validation shows that the model is reasonably well calibrated.},
author = {Azose, Jonathan J and {\v{S}}ev{\v{c}}{\'{i}}kov{\'{a}}, Hana and Raftery, Adrian E},
doi = {10.1073/pnas.1606119113},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Azose, {\v{S}}ev{\v{c}}{\'{i}}kov{\'{a}}, Raftery - 2016 - Probabilistic population projections with migration uncertainty(2).pdf:pdf},
isbn = {0951-6328},
issn = {14716925},
journal = {Proceedings of the National Academy of Sciences},
number = {23},
pages = {6460--6465},
pmid = {12869455},
title = {{Probabilistic population projections with migration uncertainty}},
url = {http://www.pnas.org/content/113/23/6460.abstract},
volume = {113},
year = {2016}
}
@article{Dodd2018,
author = {Dodd, Erengul and Forster, Jonathan J. and Bijak, Jakub and Smith, Peter W. F.},
doi = {10.1111/rssa.12309},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dodd et al. - 2018 - Smoothing mortality data the English Life Tables , 2010-2012.pdf:pdf},
issn = {09641998},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
keywords = {bayesian analysis,generalized additive model,graduation,model averaging},
title = {{Smoothing mortality data: the English Life Tables , 2010-2012}},
url = {http://doi.wiley.com/10.1111/rssa.12309},
volume = {Early View},
year = {2018}
}
@article{Brouhns2002,
author = {Brouhns, Natacha and Denuit, Michel and Vermunt, Jeroen K},
doi = {DOI: 10.1016/S0167-6687(02)00185-3},
issn = {0167-6687},
journal = {Insurance: Mathematics and Economics},
keywords = {Life insurance},
number = {3},
pages = {373--393},
title = {{A Poisson log-bilinear regression approach to the construction of projected lifetables}},
volume = {31},
year = {2002}
}
@techreport{UNPD2015,
address = {New York},
author = {{United Nations Population Division}},
institution = {United Nations},
title = {{World Population Prospects: The 2015 Revision}},
url = {https://esa.un.org/unpd/wpp/},
year = {2015}
}
@unpublished{CMI2015b,
address = {London},
author = {{Continuous Mortality Investigation}},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Continuous Mortality Investigation - 2015 - Initial report on the features of high age mortality(2).pdf:pdf},
institution = {Continuous Mortality Investigation High Age Mortality Working Party},
pages = {85},
series = {CMI Working Paper},
title = {{Initial report on the features of high age mortality}},
year = {2015}
}
@article{Bennett2015,
author = {Bennett, James E and Li, Guangquan and Foreman, Kyle and Best, Nicky and Kontis, Vasilis and Pearson, Clare and Hambly, Peter and Ezzati, Majid},
doi = {10.1016/S0140-6736(15)60296-3},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bennett et al. - 2015 - The future of life expectancy and life expectancy inequalities in England and Wales Bayesian spatiotemporal f(2).pdf:pdf},
issn = {0140-6736},
journal = {The Lancet},
number = {15},
pages = {163--170},
publisher = {Bennett et al. Open Access article distributed under the terms of CC BY},
title = {{The future of life expectancy and life expectancy inequalities in England and Wales : Bayesian spatiotemporal forecasting}},
url = {http://dx.doi.org/10.1016/S0140-6736(15)60296-3},
volume = {6736},
year = {2015}
}
@book{Thatcher1998,
address = {Odense},
author = {Thatcher, A. F. and Kannisto, V. and Vaupel, James W.},
publisher = {Odense University Press},
title = {{The force of mortality at ages 80-120}},
url = {http://www.demogr.mpg.de/Papers/Books/Monograph5/ForMort.htm},
year = {1998}
}
@book{Wood2006,
address = {Boca Raton},
author = {Wood, Simon N.},
publisher = {Chapman and Hall / CRC Press},
title = {{Generalised Additive Models: An Introduction with R}},
year = {2006}
}
@article{Li2013a,
author = {Li, Nan and Lee, Ronald and Gerland, Patrick},
doi = {10.1007/s13524-013-0232-2.Extending},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Lee, Gerland - 2015 - HHS Public Access.pdf:pdf},
journal = {Demography2},
keywords = {public access},
number = {6},
pages = {2037--2051},
title = {{Extending the Lee-Carter method to model the rotation of age patterns of mortality decline for long-term projection}},
volume = {50},
year = {2013}
}
@unpublished{Lang2001,
address = {Munich},
author = {Lang, Stefan and Brezger, Andreas},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lang, Brezger - 2001 - Bayesian P-Splines(2).pdf:pdf},
institution = {Ludwig-Maximilians-Universit{\"{a}}t-M{\"{u}}nchen},
number = {236},
series = {Insitut F{\"{u}}r Statistik Sonderforschungsbereich 386},
title = {{Bayesian P-Splines}},
url = {http://epub.ub.uni-muenchen.de/},
year = {2001}
}
@article{Suzman1985,
author = {Suzman, Richard and {White Riley}, Matilda},
journal = {The Millbank Memorial Fund Quarterly. Health and Soceity},
number = {2},
pages = {175--186},
title = {{Introducing the Oldest-Old}},
volume = {63},
year = {1985}
}
@article{BMS2002,
author = {Booth, Heather and Maindonald, John and Smith, Len},
issn = {00324728},
journal = {Population Studies},
number = {3},
pages = {pp. 325--336},
publisher = {Population Investigation Committee},
title = {{Applying Lee-Carter under Conditions of Variable Mortality Decline}},
url = {http://www.jstor.org/stable/3092985},
volume = {56},
year = {2002}
}
@techreport{Dodd2017,
author = {Dodd, Erengul and Forster, Jonathan J. and Bijak, Jakub and Smith, Peter W.F.},
institution = {ESRC Centre for Population Change},
title = {{Methodology Review for the Mortality Assumptions in the UK National Population Projections}},
year = {2017}
}
@article{Cairns2011,
abstract = {The paper introduces a new framework for modelling the joint development over time of mortality rates in a pair of related populations by combining a number of recent and novel developments in stochastic mortality modelling. First, we develop an underlying stochastic model which incorporates a mean-reverting stochastic spread that allows for different trends in mortality improvement rates in the short-run, but parallel improvements in the long run in line with the principles of biological reasonableness. Second, we fit the model using a Bayesian framework that allows us to combine estimation of the unobservable state variables and the parameters of the stochastic processes driving them into a single procedure. This procedure employs Markov chain Monte Carlo (MCMC) techniques, permitting us to analyse uncertainty in the estimates of the historical age, period and cohort effects, and this helps us to smooth out noise in parameter estimates attributable to small populations. Mortality rates arising from this framework provide consistent forecasts for the two populations. Further, estimated correlations based on the simulated mortality improvement factors for two populations are consistent with historical data. The framework is illustrated using two-population extensions of the Age-Period-Cohort and Lee-Carter models on the following populations: England {\&} Wales national and CMI assured lives males and females, and US and California males. The approach is designed for large populations coupled with a small sub-population, but is easily adaptable to other combinations. A key application of the modelling framework would be to allow longevity risk hedgers to model the basis risk that exists between mortality rates in two populations in the case where the hedger wishes to hedge the risk in one population using a hdging instrument based on the second population.},
author = {Cairns, Andrew J. G. and Blake, David and Dowd, Kevin and Coughlan, Guy D. and Khalaf-Allah, Marwa},
doi = {10.2143/AST.41.1.2084385},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cairns et al. - 2011 - Bayesian stochastic mortality modelling for two populations(2).pdf:pdf},
issn = {05150361},
journal = {ASTIN Bulletin},
keywords = {2011,a,age effect,and khalaf-allah,bayesian stochastic mortality modelling,blake,cairns,chain monte carlo,cohort effect,coughlan,d,dowd,for two populations,g,j,k,m,markov,missing data,parameter uncertainty,period effect,small sub-populations,suggested citation,to appear in astin},
number = {1},
pages = {29--59},
title = {{Bayesian stochastic mortality modelling for two populations}},
volume = {41},
year = {2011}
}
@inproceedings{Beard1963,
address = {New York},
author = {Beard, R.E.},
booktitle = {Proceedings of the International Population Conference1},
pages = {611--625},
publisher = {International Union for the Scientific Study of Population},
title = {{A theory of mortality based on actuarial, biological, and medical considerations.}},
year = {1963}
}
@article{Spiegelhalter2002,
author = {Spiegelhalter, David J and Best, Nicola G. and Carlin, Bradley P and var der Linder, Angelika},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Spiegelhalter, Carlin - 2002 - Bayesian Measures of Model Complexity and Fit Author ( s ) David J . Spiegelhalter , Nicola G . Best ,(2).pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
number = {4},
pages = {583--639},
title = {{Bayesian Measures of Model Complexity and Fit}},
volume = {64},
year = {2002}
}
@article{Li2013,
abstract = {We examine the application of a Poisson common factor model for the projection of mortality jointly for females and males. The model structure is an extension of the classical Lee-Carter method in which there is a common factor for the aggregate population, while a number of additional sex-specific factors can also be incorporated. The Poisson distribution is a natural choice for modelling the number of deaths, and its use provides a formal statistical framework for model selection, parameter estimation, and data analysis. Our results for Australian data show that this model leads to projected life expectancy values similar to those produced by the separate projection of mortality for females and males, but possesses the additional advantage of ensuring that the projected male-to-female ratio for death rates at each age converges to a constant. Moreover, the randomness of the corresponding residuals indicates that the model fit is satisfactory.},
author = {Li, Jackie},
doi = {10.1080/00324728.2012.689316},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2013 - A Poisson common factor model for projecting mortality and life expectancy jointly for females and males.pdf:pdf},
issn = {0032-4728},
journal = {Population Studies},
keywords = {2011,australian mortality,death rates of both,final version accepted october,poisson common factor model,sexes,submitted november 2010},
number = {1},
pages = {111--126},
pmid = {22788919},
title = {{A Poisson common factor model for projecting mortality and life expectancy jointly for females and males}},
volume = {67},
year = {2013}
}
@book{Keyfitz2005,
address = {New York},
author = {Keyfitz, Nathan and Caswell, Hal},
edition = {Third},
publisher = {Springer},
title = {{Applied Mathematical Demography}},
year = {2005}
}
@article{Gneiting2007a,
author = {Gneiting, Tilmann and Raftery, Adrian E.},
doi = {10.1198/016214506000001437},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation(2).pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {prediction,strictly proper scoring rules},
number = {477},
title = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
volume = {402},
year = {2007}
}
@misc{StanDevelopmentTeam2015,
author = {{Stan Development Team}},
title = {{Stan Modeling Language Users Guide and Reference Manual}},
url = {http://mc-stan.org/index.html},
year = {2015}
}
@article{Vaupel1979,
abstract = {Life table methods are developed for populations whose members differ in their endowment for longevity. Unlike standard methods, which ignore such heterogeneity, these methods use different calculations to construct cohort, period, and individual life tables. The results imply that standard methods overestimate current life expectancy and potential gains in life expectancy from health and safety interventions, while underestimating rates of individual aging, past progress in reducing mortality, and mortality differentials between pairs of populations. Calculations based on Swedish mortality data suggest that these errors may be important, especially in old age.},
author = {Vaupel, James W. and Manton, Kenneth G. and Stallard, Eric},
doi = {10.2307/2061224},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaupel, Manton, Stallard - 1979 - The Impact of Heterogeneity in Individual Frailty on the Dynamics of Mortality.pdf:pdf},
isbn = {00703370},
issn = {00703370},
journal = {Demography},
number = {3},
pages = {439},
pmid = {510638},
title = {{The Impact of Heterogeneity in Individual Frailty on the Dynamics of Mortality}},
url = {http://link.springer.com/10.2307/2061224},
volume = {16},
year = {1979}
}
@article{rhageenhance,
author = {Renshaw, A and Haberman, S},
journal = {Insurance: Mathematics and Economics},
pages = {255--272},
title = {{Lee-Carter mortality forecasting with age-specific enhancement}},
volume = {33},
year = {2003}
}
@techreport{Borger2014,
abstract = {The projection of future mortality experience constitutes a challenge for both demographers and actuaries. As we show, some of the currently used projections have several shortcomings which may pose a serious threat to social security systems, pension funds, and insurers. Therefore, in this paper, we propose a new projection methodology which overcomes these shortcomings. Our model allows mortality improvements to depend on age, period, and cohort and provides highly plausible forecasts. We explain how the model can be calibrated and how typical issues in the projection of mortality, e.g. the forecast of mortality improvements for very old ages, can be solved within our modeling framework. Moreover, our model is very flexible with respect to the level of future mortality improvements. This allows us to derive coherent projections for several populations, i.e. males and females of the same country and populations from closely related countries. The basis for these projections are coherent extrapolations of historical life expectancies which, as aggregated mortality statistics, exhibit rather steady patterns. We observe that the incorporation of information about the mortality experience of other populations can have a significant impact on the projection for a given population. In order to illustrate our methodology, we derive fully specified projections for German males and females as members of a large reference set of European populations. Finally, we discuss uncertainties in our modeling framework, e.g. model uncertainty, parameter uncertainty or basis risk, and show how they can be assessed.},
address = {Ulm},
author = {B{\"{o}}rger, Matthias and Aleksic, Marie-Christine},
booktitle = {Mimeo},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{o}}rger, Aleksic - 2014 - Coherent Projections of Age , Period , and Cohort Dependent Mortality Improvements(2).pdf:pdf},
institution = {Institute for Financial and Acturial Sciences, University of Ulm},
pages = {1--26},
title = {{Coherent Projections of Age , Period , and Cohort Dependent Mortality Improvements}},
year = {2014}
}
@article{leecarter1992,
author = {Lee, Ronald D and Carter, Lawrence R},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {419},
pages = {pp. 659--671},
publisher = {American Statistical Association},
title = {{Modeling and Forecasting U.S Mortality}},
url = {http://www.jstor.org/stable/2290201},
volume = {87},
year = {1992}
}
@misc{rman,
address = {Vienna, Austria},
annote = {{\{}ISBN{\}} 3-900051-07-0},
author = {{R Development Core Team}},
institution = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {http://www.r-project.org/},
year = {2017}
}
@article{Currie2016,
abstract = {Many common models of mortality can be expressed compactly in the language of either generalized linear models or generalized non-linear models. The R language provides a description of these models which parallels the usual algebraic definitions but has the advantage of a transparent and flexible model specification. We compare eight model structures for mortality. For each structure, we consider (a) the Poisson models for the force of mortality with both log and logit link functions and (b) the binomial models for the rate of mortality with logit and complementary logâ€“log link functions. Part of this work shows how to extend the usual smooth two-dimensional P-spline model for the force of mortality with Poisson error and log link to the other smooth two-dimensional P-spline models with Poisson and binomial errors defined in (a) and (b). Our comments are based on the results of fitting these models to data from six countries:Australia, France, Japan, Sweden, UK and USA. We also discuss the possibility of forecasting with these models; in particular, the introduction of cohort terms generally leads to an improvement in overall fit, but can also make forecasting with these models problematic.},
author = {Currie, Iain D.},
doi = {10.1080/03461238.2014.928230},
file = {:C$\backslash$:/mendeleydump/On fitting generalized linear and non linear models of mortality.pdf:pdf},
issn = {0346-1238},
journal = {Scandinavian Actuarial Journal},
keywords = {constraints,generalized linear models,identifiability,mortality,r language},
pages = {356--383},
publisher = {Taylor {\&} Francis},
title = {{On fitting generalized linear and non-linear models of mortality}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03461238.2014.928230},
volume = {4},
year = {2016}
}
@article{Kontis2017a,
abstract = {Background Projections of future mortality and life expectancy are needed to plan for health and social services and pensions. Our aim was to forecast national age-specifi c mortality and life expectancy using an approach that takes into account the uncertainty related to the choice of forecasting model.},
author = {Kontis, Vasilis and Bennett, James E and Mathers, Colin D and Li, Guangquan and Foreman, Kyle and Ezzati, Majid},
doi = {10.1016/S0140-6736(16)32381-9},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kontis et al. - 2017 - Future life expectancy in 35 industrialised countries projections with a Bayesian model ensemble(2).pdf:pdf},
issn = {0140-6736},
journal = {The Lancet},
number = {16},
pages = {1--13},
publisher = {The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY license},
title = {{Future life expectancy in 35 industrialised countries: projections with a Bayesian model ensemble}},
url = {http://dx.doi.org/10.1016/S0140-6736(16)32381-9},
volume = {6736},
year = {2017}
}
@article{Enchev2016,
abstract = {We review a number of multi-population mortality models: variations of the Li {\&} Lee model, and the common-age-effect (CAE) model of Kleinow. Model parameters are estimated using maximum likelihood. Although this introduces some challenging identifiability problems and complicates the estimation process it allows a fair comparison of the different models. We propose to solve these identifiability problems by applying two- dimensional constraints over the parameters. Using data from six countries, we compare and rank, both visually and numerically, the models' fitting qualities and develop forecasting models that produce non-diverging, joint mortality rate scenarios. It is found that the CAE model fits best. But we also find that the Li and Lee model potentially suffers from robustness problems when calibrated using maximum likelihood.},
author = {Enchev, Vasil and Kleinow, Torsten and Cairns, Andrew J. G.},
doi = {10.1080/03461238.2015.1133450},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Enchev, Kleinow, Cairns - 2016 - Multi-population mortality models fitting, forecasting and comparisons.pdf:pdf},
issn = {0346-1238},
journal = {Scandinavian Actuarial Journal},
keywords = {Li and Lee model,common age effect model,multi-population,stochastic mortality model},
number = {February},
pages = {1--24},
publisher = {Taylor {\&} Francis},
title = {{Multi-population mortality models: fitting, forecasting and comparisons}},
url = {http://www.tandfonline.com/doi/full/10.1080/03461238.2015.1133450},
volume = {1238},
year = {2016}
}
@unpublished{Richards2017,
address = {Edinburgh},
author = {Richards, Stephen J. and Currie, Iain D and Kleinow, Torsten and Ritchie, Gavin P},
file = {:C$\backslash$:/mendeleydump/APCI{\_}booklet{\_}v2.pdf:pdf},
institution = {Longevitas Ltd},
keywords = {apc,arima models,cte,lee-carter,mortality projections,solvency ii,var},
title = {{A Stochastic implementation of the APCI model for mortality projections}},
url = {https://www.longevitas.co.uk/site/apci/apci.html},
year = {2017}
}
@article{Yao2017,
abstract = {The widely recommended procedure of Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.},
archivePrefix = {arXiv},
arxivId = {1704.02030},
author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
eprint = {1704.02030},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2017 - Using stacking to average Bayesian predictive distributions(2).pdf:pdf},
journal = {arXiv},
keywords = {bayesian model averaging,distribution,model combination,predictive,proper scoring rule,stacking,stan},
title = {{Using stacking to average Bayesian predictive distributions}},
url = {http://arxiv.org/abs/1704.02030},
volume = {1704.02030},
year = {2017}
}
@article{Chan2014,
author = {Chan, Wai-Sum and Li, Johnny Siu-Hang and Li, Jackie},
doi = {10.1080/10920277.2013.854161},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Li, Li - 2014 - The CBD Mortality Indexes Modeling and Applications.pdf:pdf},
issn = {1092-0277},
journal = {North American Actuarial Journal},
keywords = {cairns-blake-dowd model,prediction regions,varima models},
number = {March 2015},
pages = {38--58},
title = {{The CBD Mortality Indexes: Modeling and Applications}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10920277.2013.854161},
volume = {18},
year = {2014}
}
@incollection{Neal2010,
author = {Neal, Radford},
booktitle = {Handbook of Markov Chain Monte Carlo},
editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li Meng},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 2010 - MCMC using Hamiltonian Dynamics(2).pdf:pdf},
publisher = {Chapman and Hall / CRC Press},
title = {{MCMC using Hamiltonian Dynamics}},
year = {2010}
}
@article{Kontis2017,
author = {Kontis, Vasilis and Bennett, James E and Mathers, Colin D and Li, Guangquan and Foreman, Kyle and Ezzati, Majid},
doi = {10.1056/NEJMoa1506699},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kontis et al. - 2017 - Supplementary Appendix to Future life expectancy in 35 industrialised countries projections with a Bayesian mo(2).pdf:pdf},
isbn = {0000030449},
issn = {1533-4406},
journal = {The Lancett},
number = {16},
pmid = {22316447},
title = {{Supplementary Appendix to Future life expectancy in 35 industrialised countries: projections with a Bayesian model ensemble}},
volume = {6736},
year = {2017}
}
@article{McNown1989,
abstract = {This article links parameterized model mortality schedules with time
series methods to develop forecasts of U.S. mortality to the year
2000. The use of model mortality schedules permits a relatively concise
representation of the history of mortality by age and sex from 1900
to 1985, and the use of modern time series methods to extend this
history forward to the end of this century allows for a flexible
modeling of trend and the accommodation of changes in long-run mortality
patterns. This pilot study demonstrates that the proposed procedure
produces medium-range forecasts of mortality that meet the standard
tests of accuracy in forecast evaluation and that are sensible when
evaluated against the comparable forecasts produced by the Social
Security Administration.},
author = {McNown, Robert and Rogers, Andrei},
issn = {00703370},
journal = {Demography},
number = {4},
pages = {pp. 645--660},
publisher = {Springer on behalf of the Population Association of America},
title = {{Forecasting Mortality: A Parameterized Time Series Approach}},
url = {http://www.jstor.org/stable/2061263},
volume = {26},
year = {1989}
}
@article{Bryant2015,
author = {Bryant, John R and Graham, Patrick J.},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bryant, Graham - 2015 - A Bayesian Approach to Population Estimation with Administrative Data(2).pdf:pdf},
journal = {Journal of Official Statistics},
keywords = {administrative data,bayesian,demography,official statistics},
number = {3},
pages = {475--487},
title = {{A Bayesian Approach to Population Estimation with Administrative Data}},
volume = {31},
year = {2015}
}
@unpublished{CMI2016,
author = {{Continuous Mortality Investigation}},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Continuous Mortality Investigation - 2016 - CMI Mortality Projections Model consultation(2).pdf:pdf},
institution = {Continuous Mortality Investigation Mortality Projections Committee},
number = {August},
pages = {1--50},
series = {CMI working paper},
title = {{CMI Mortality Projections Model consultation}},
url = {https://www.actuaries.org.uk/documents/cmi-working-paper-90-cmi-mortality-projections-model-consultation},
year = {2016}
}
@techreport{Cridland2017,
address = {London},
author = {Cridland, John},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cridland - 2017 - Smoothing the Transition Independent Review of the State Pension Age(2).pdf:pdf},
isbn = {9781474142014},
title = {{Smoothing the Transition: Independent Review of the State Pension Age}},
url = {https://www.gov.uk/government/publications/state-pension- age-independent-review-final-report{\%}0AAny},
year = {2017}
}
@article{Lee2000,
author = {Lee, Ronald},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee - 2000 - THE LEE-CARTER METHOD FOR FORECASTING MORTALITY , WITH VARIOUS EXTENSIONS AND APPLICATIONS(2).pdf:pdf},
journal = {North American Actuarial Journal},
keywords = {Demography,Forecasting,Lee-Carter Method,Mortality},
mendeley-tags = {Demography,Forecasting,Lee-Carter Method,Mortality},
number = {1},
pages = {80--93},
title = {{THE LEE-CARTER METHOD FOR FORECASTING MORTALITY , WITH VARIOUS EXTENSIONS AND APPLICATIONS}},
volume = {4},
year = {2000}
}
@unpublished{Dodd,
author = {Dodd, Erengul and Forster, Jonathan J. and Bijak, Jakub and Smith, Peter W. F.},
title = {{Stochastic Modelling and Projection of Mortality Improvements Allowing for Overdispersion}},
year = {2017}
}
@article{booth2006,
author = {Booth, Heather and Hyndman, Rob and Tickle, Leonie and de Jong, Piet},
doi = {10.4054/DemRes.2006.15.9},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Booth et al. - 2006 - Lee-Carter mortality forecasting a multi-country comparison of variants and extensions(2).pdf:pdf},
journal = {Demographic Research},
number = {9},
pages = {289--310},
title = {{Lee-Carter mortality forecasting: a multi-country comparison of variants and extensions}},
url = {http://www.demographic-research.org/volumes/vol15/9/},
volume = {15},
year = {2006}
}
@article{Wood2011,
abstract = {Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton-Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
author = {Wood, Simon N.},
doi = {10.1111/j.1467-9868.2010.00749.x},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood - 2011 - Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear model(2).pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Adaptive smoothing,Generalized additive mixed model,Generalized additive model,Generalized cross-validation,Marginal likelihood,Model selection,Penalized generalized linear model,Penalized regression splines,Restricted maximum likelihood,Scalar on function regression,Stable computation},
number = {1},
pages = {3--36},
title = {{Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models}},
volume = {73},
year = {2011}
}
@article{Hyndman2007,
abstract = {A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age, is robust for outlying years due to wars and epidemics, and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis, nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error. The model is a generalization of the Lee-Carter (LC) model commonly used in mortality and fertility forecasting. The methodology is applied to French mortality data and Australian fertility data, and the forecasts obtained are shown to be superior to those from the LC method and several of its variants. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Hyndman, Rob J. and Ullah, Md. Shahid},
doi = {10.1016/j.csda.2006.07.028},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyndman, Shahid Ullah - 2007 - Robust forecasting of mortality and fertility rates A functional data approach(2).pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Fertility forecasting,Functional data,Mortality forecasting,Nonparametric smoothing,Principal components,Robustness},
number = {10},
pages = {4942--4956},
title = {{Robust forecasting of mortality and fertility rates: A functional data approach}},
volume = {51},
year = {2007}
}
@article{Vehtari2015,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We compute LOO using Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {1507.04544},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
eprint = {1507.04544},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vehtari, Gelman, Gabry - 2015 - Efficient implementation of leave-one-out cross-validation and WAIC for evaluating fitted Bayesian mo(2).pdf:pdf},
keywords = {bayesian computation,k -fold cross-validation,leave-one-out cross-validation,loo,pareto smoothed importance sampling,stan,waic,widely applicable information criterion},
number = {July},
title = {{Efficient implementation of leave-one-out cross-validation and WAIC for evaluating fitted Bayesian models}},
url = {http://arxiv.org/abs/1507.04544},
year = {2015}
}
@article{Renshaw2003a,
author = {Renshaw, A and Haberman, S},
journal = {Insurance: Mathematics and Economics},
pages = {379--401},
title = {{On the forecasting of mortality reduction factors}},
volume = {32},
year = {2003}
}
@article{Ohare2017,
author = {O'hare, Colin and Li, Youwei},
doi = {10.1080/00036846.2017.1305092},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'hare, Li - 2017 - Models of mortality rates â€“ analysing the residuals.pdf:pdf},
issn = {0003-6846},
journal = {Applied Economics},
keywords = {0003-6846,1466-4283,Hurst exponents,Mortality,analysing the residuals,colin o,com,hare,http,issn,journal homepage,lied economics,loi,models of mortality rates,online,print,raec20,residuals,stochastic models,tandfonline,www,youwei li},
number = {00},
pages = {1--15},
publisher = {Routledge},
title = {{Models of mortality rates â€“ analysing the residuals}},
url = {https://www.tandfonline.com/doi/full/10.1080/00036846.2017.1305092},
volume = {00},
year = {2017}
}
@article{Schoen2016,
abstract = {Hierarchical models are characterized by having N living states connected by N âˆ’ 1 rates of transfer. Demographic measures for such models can be calculated directly from counts of the number of persons in each state at two nearby points in time. Exploiting the ability of population stocks to determine the flows in hierarchical models expands the range of demographic analysis. The value of such analyses is illustrated by an application to childbearing, where the states of interest reflect the number of children a woman has born. Using Census data on the distribution of women by age and parity, a parity status life table for US Women, 2005â€“2010, is constructed. That analysis shows that nearly a quarter of American women are likely to remain childless, with a 0â€“3 child pattern replacing the 2â€“4 child pattern of the past.},
author = {Schoen, Robert},
doi = {10.7717/peerj.2535},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schoen - 2016 - Hierarchical multistate models from population data an application to parity statuses.pdf:pdf},
issn = {21678359},
journal = {PeerJ},
keywords = {an application to parity,population data,rarchical multistate models from},
pages = {e2535},
title = {{Hierarchical multistate models from population data: an application to parity statuses}},
volume = {4},
year = {2016}
}
@article{Cairns2006,
author = {Cairns, Andrew J G and Dowd, Kevin},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cairns, Blake, Dowd - 2006 - A Two-Factor Model for Stochastic Mortality with Parameter Uncertainty(2).pdf:pdf},
journal = {Journal of Risk and Insurance},
number = {4},
pages = {687--718},
title = {{A two-factor model for stochastic mortality with parameter uncertainty: theory and calibration}},
volume = {73},
year = {2006}
}
@article{Cairns2011a,
abstract = {This paper develops a framework for developing forecasts of future mortality rates. We discuss the suitability of six stochastic mortality models for forecasting future mortality and estimating the density of mortality rates at different ages. In particular, the models are assessed individually with reference to the following qualitative criteria that focus on the plausibility of their forecasts: biological reasonableness; the plausibility of predicted levels of uncertainty in forecasts at different ages; and the robustness of the forecasts relative to the sample period used to fit the model. An important, though unsurprising, conclusion is that a good fit to historical data does not guarantee sensible forecasts. We also discuss the issue of model risk, common to many modelling situations in demography and elsewhere. We find that even for those models satisfying our qualitative criteria, there are significant differences among central forecasts of mortality rates at different ages and among the distributions surrounding those central forecasts. ?? 2011 Elsevier B.V.},
author = {Cairns, Andrew J G and Blake, David and Dowd, Kevin and Coughlan, Guy D. and Epstein, David and Khalaf-Allah, Marwa},
doi = {10.1016/j.insmatheco.2010.12.005},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cairns et al. - 2011 - Mortality density forecasts An analysis of six stochastic mortality models(2).pdf:pdf},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Fan charts,Forecasting,Model risk,Model selection criteria,Plausibility},
number = {3},
pages = {355--367},
publisher = {Elsevier B.V.},
title = {{Mortality density forecasts: An analysis of six stochastic mortality models}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2010.12.005},
volume = {48},
year = {2011}
}
@techreport{CMI2010,
address = {London},
author = {{Continuous Mortality Investigation}},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Continuous Mortality Investigation - 2010 - The CMI Mortality Projections Model, CMI{\_}2010(2).pdf:pdf},
institution = {Continuous Mortality Investigation},
series = {CMI working Paper},
title = {{The CMI Mortality Projections Model, CMI{\_}2010}},
year = {2010}
}
@article{Schoen2006,
abstract = {We present a new, broadly applicable approach to summarizing the behavior of a cohort as it moves through a variety of statuses (or states). The approach is based on the assumption that all rates of transfer maintain a constant ratio to one another over age. We present closed-form expressions for the size and state composition of the cohort at every age and provide expressions for other useful summary measures. The state trajectories, or life course schematics, depict all the possible size and state configurations that the cohort can exhibit over its life course under the specified pattern of transfer rates. The two living state case and hierarchical multistate models with any number of living states are analyzed in detail. Applying our approach to 1997 U.S. fertility data, we find that observed rates of parity progression are roughly proportional over age. Our proportional transfer rate approach provides trajectories by parity state and facilitates analyses of the implications of changes in parity rate levels and patterns. More women complete childbearing at parity 2 than at any other parity, and parity 2 would be the modal parity in models with total fertility rates (TFRs) of 1.40 to 2.61. Increases in parity progression rates to parities 4 and above have little effect on a cohort's TFR, while changes in childlessness have a substantial impact.},
author = {Schoen, Robert and Canudas-Romo, Vladimir},
doi = {10.1353/dem.2006.0027},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schoen, Canudas-Romo - 2006 - Multistate cohort models with proportional transfer rates.pdf:pdf},
issn = {0070-3370},
journal = {Demography},
keywords = {adult,cohort studies,female,humans,male,middle aged,models,residential mobility,statistical,united states},
number = {3},
pages = {553--568},
pmid = {17051827},
title = {{Multistate cohort models with proportional transfer rates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17051827},
volume = {43},
year = {2006}
}
@article{Currie2006,
author = {Currie, I D and Durb{\'{a}}n, M and Eilers, P H C},
file = {:C$\backslash$:/mendeleydump/Currie{\_}et{\_}al-2006-Journal{\_}of{\_}the{\_}Royal{\_}Statistical{\_}Society-{\_}Series{\_}B{\_}(Statistical{\_}Methodology).pdf:pdf},
journal = {Journal of the Royal Statististical Society, Series B},
keywords = {2006,259,280,68,b,generalized linear array models,multidimensional smoothing,part 2,pp,r,soc,statist,with applications to},
pages = {1--22},
title = {{Generalized linear array models with applications to multidimensional somoothing}},
volume = {68},
year = {2006}
}
@techreport{OfficeforNationalStatistics2016c,
author = {{Office for National Statistics}},
file = {:C$\backslash$:/mendeleydump/National Population Projections 2014-based reference volume, series PP2.pdf:pdf},
institution = {Office for National Statistics},
keywords = {2014-based,an indication of the,future size and age,national population projections,national population projections provide,pendium,reference volume,series pp2,structure},
title = {{National Population Projections: 2014-based Reference Volume, Series PP2}},
url = {https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationprojections\\/compendium/nationalpopulationprojections/2014basedreferencevolumeseriespp2},
volume = {PP2},
year = {2016}
}
@article{Currie2004,
author = {Currie, Iain D and Durban, Maria and Eilers, Paul H C},
file = {:C$\backslash$:/mendeleydump/1471082x04st080oa.pdf:pdf},
journal = {Statistical Modelling},
pages = {279--298},
title = {{Smoothing and forecasting mortality rates}},
volume = {4},
year = {2004}
}
@article{Raftery2016,
abstract = {The analysis of climate data has relied heavily on hypothesis-driven statistical methods, while projections of future climate are based primarily on physics-based computational models. However, in recent years a wealth of new datasets has become available. Therefore, we take a more data-centric approach and propose a unified framework for studying climate, with an aim toward characterizing observed phenomena as well as discovering new knowledge in climate science. Specifically, we posit that complex networks are well suited for both descriptive analysis and predictive modeling tasks. We show that the structural properties of 'climate networks' have useful interpretation within the domain. Further, we extract clusters from these networks and demonstrate their predictive power as climate indices. Our experimental results establish that the network clusters are statistically significantly better predictors than clusters derived using a more traditional clustering approach. Using complex networks as data representation thus enables the unique opportunity for descriptive and predictive modeling to inform each other. 2010 Wiley Periodicals, Inc.},
archivePrefix = {arXiv},
arxivId = {1206.3552},
author = {Raftery, Adrian E.},
doi = {10.1002/sam.11302},
eprint = {1206.3552},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery - 2016 - Use and communication of probabilistic forecasts(2).pdf:pdf},
isbn = {1932-1872},
issn = {19321872},
journal = {Statistical Analysis and Data Mining},
keywords = {calibration,cognitive research,decision theory,risk,uncertainty},
number = {6},
pages = {397--410},
pmid = {21824845},
title = {{Use and communication of probabilistic forecasts}},
volume = {9},
year = {2016}
}
@book{Gelman2014a,
address = {Abingdon},
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
edition = {Third},
publisher = {CRC Press},
title = {{Bayesian Data Analysis}},
year = {2014}
}
@article{Mayhew2013,
abstract = {There is increasing concern about the lack of accuracy in population projections at national levels. A common problem has been the systematic underestimation of improvements in mortality, especially at older ages, resulting in projections that are too low. In this paper, we present a method that is based on projecting survivorship rather than mortality, which uses the same data but differs technically. In particular, rather than extrapolating trends in mortality, we use trends in life expectancy to establish a robust statistical relation between changes in life expectancy and survivorship using period life tables. We test the approach on data for England and Wales for the population aged 50 and over, and show that it gives more accurate projections than official projections using the same base data. Using the model to project the population aged 50 and over to 2020, our method suggests nearly 0.6 million more people in this age group than official projections.},
author = {Mayhew, Les and Smith, David},
doi = {10.1080/00324728.2012.740500},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayhew, Smith - 2013 - A new method of projecting populations based on trends in life expectancy and survival.pdf:pdf},
issn = {1477-4747},
journal = {Population studies},
keywords = {Aged,England,Female,Forecasting,Forecasting: methods,Humans,Life Expectancy,Life Expectancy: trends,Male,Middle Aged,Survival Analysis,Survival Rate,Survival Rate: trends,Wales},
number = {2},
pages = {157--70},
pmid = {23199204},
title = {{A new method of projecting populations based on trends in life expectancy and survival.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23199204},
volume = {67},
year = {2013}
}
@article{denuit,
author = {Denuit, Michel},
journal = {Lifetime data analysis},
pages = {381--397},
title = {{Distribution of the random future life expectancies in log-bilinear mortality projection models.}},
volume = {13},
year = {2007}
}
@inproceedings{Forster2016,
address = {Geneva},
author = {Forster, Jonathan J. and Dodd, Erengul and Bijak, Jakub and Smith, Peter W. F.},
booktitle = {Joint Eurostat/UNECE Work Session on Demographic Projections},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Forster et al. - 2016 - A Comprehensive Framework for Mortality Forecasting.pdf:pdf},
title = {{A Comprehensive Framework for Mortality Forecasting}},
year = {2016}
}
@article{Bijak2015b,
author = {Bijak, Jakub and Alberts, Isabel and Alho, Juha and Bryant, John and Buettner, Thomas and Falkingham, Jane and Forster, Jonathan J. and Gerland, Patrick and King, Thomas and Onorante, Luca and Keilman, Nico and O'Hagan, Anthony and Owens, Darragh and Raftery, Adrian and Sevcikova, Hana and Smith, Peter W. F.},
doi = {10.1037/emo0000122.Do},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bijak et al. - 2015 - Letter to the Editor Probabilistic population forecasts for informed decision making(2).pdf:pdf},
journal = {Journal of Official Statistics},
number = {4},
pages = {537--544},
title = {{Letter to the Editor: Probabilistic population forecasts for informed decision making}},
volume = {31},
year = {2015}
}
@article{Golden2016,
author = {Golden, Linda L. and Brockett, Patrick L. and Ai, Jing and Kellison, Bruce},
doi = {10.1080/10920277.2016.1209118},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Golden et al. - 2016 - Empirical Evidence on the Use of Credit Scoring for Predicting Insurance Losses with Psycho-social and Biochemica.pdf:pdf},
issn = {1092-0277},
journal = {North American Actuarial Journal},
keywords = {1092-0277,2325-0453,com,empirical evidence on the,for,http,issn,journal homepage,loi,online,predicting insurance losses with,print,psycho-social and,tandfonline,th american actuarial journal,uaaj20,use of credit scoring,www},
number = {3},
pages = {233--251},
title = {{Empirical Evidence on the Use of Credit Scoring for Predicting Insurance Losses with Psycho-social and Biochemical Explanations}},
url = {https://www.tandfonline.com/doi/full/10.1080/10920277.2016.1209118},
volume = {20},
year = {2016}
}
@article{Renshaw2003,
annote = {Papers presented at the 6th IME Conference, Lisbon, 15-17 July 2002},
author = {Renshaw, A E and Haberman, S},
doi = {DOI: 10.1016/S0167-6687(03)00138-0},
issn = {0167-6687},
journal = {Insurance: Mathematics and Economics},
keywords = {Mortality forecasting},
number = {2},
pages = {255--272},
title = {{Lee-Carter mortality forecasting with age-specific enhancement}},
url = {http://www.sciencedirect.com/science/article/pii/S0167668703001380},
volume = {33},
year = {2003}
}
@article{Renshaw2006,
author = {Renshaw, A and Haberman, S},
journal = {Insurance: Mathematics and Economics},
pages = {556--570},
title = {{A cohort-based extension to the Lee-Carter model for mortality reduction factors}},
volume = {38},
year = {2006}
}
@article{Kleinow2017,
abstract = {The projection of mortality rates is an essential part of valuing liabilities in life insurance portfolios and pension schemes. An important tool for risk management and solvency purposes is a stochastic projection model. We show that ARIMA models can be better representations of mortality time-series than simple random-walk models. We also consider the issue of parameter risk in time-series models from the point of view of an insurer using them for regulatory risk reporting â€“ formulae are given for decomposing overall risk into undiversifiable trend risk (parameter uncertainty) and diversifiable volatility. Particular attention is given to the contrasts in how academic researchers might view these models and how insurance regulators and practitioners in life offices might use them. Using a bootstrapmethod we find that, while certain kinds of parameter risk are negligible, others are too material to ignore. We also find that an objective model selection criterion, such as goodness of fit to past data, can result in the selection of a model with unstable parameter values. While this aspect of the model is superficially undesirable, it also leads to slightly higher capital requirements and thus makes the model of keen interest to regulators. Our conclusions have relevance to insurers using value-at- risk capital assessments in the European Union under Solvency II, but also territories using conditional tail expectations such as Australia, Canada and Switzerland.},
author = {Kleinow, Torsten and Richards, Stephen J.},
doi = {10.1080/03461238.2016.1255655},
file = {:C$\backslash$:/mendeleydump/Parameter risk in time series mortality forecasts (1).pdf:pdf},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {ARIMA models,CTE,Mortality projections,Solvency II,VaR,parameter risk},
pages = {804--828},
publisher = {Taylor {\&} Francis},
title = {{Parameter risk in time-series mortality forecasts}},
url = {http://dx.doi.org/10.1080/03461238.2016.1255655},
volume = {9},
year = {2017}
}
@article{Gillespie2014,
abstract = {In the past six decades, lifespan inequality has varied greatly within and among countries even while life expectancy has continued to increase. How and why does mortality change generate this diversity? We derive a precise link between changes in age-specific mortality and lifespan inequality, measured as the variance of age at death. Key to this relationship is a young-old threshold age, below and above which mortality decline respectively decreases and increases lifespan inequality. First, we show for Sweden that shifts in the threshold's location have modified the correlation between changes in life expectancy and lifespan inequality over the last two centuries. Second, we analyze the post-World War II (WWII) trajectories of lifespan inequality in a set of developed countries-Japan, Canada, and the United States-where thresholds centered on retirement age. Our method reveals how divergence in the age pattern of mortality change drives international divergence in lifespan inequality. Most strikingly, early in the 1980s, mortality increases in young U.S. males led to a continuation of high lifespan inequality in the United States; in Canada, however, the decline of inequality continued. In general, our wider international comparisons show that mortality change varied most at young working ages after WWII, particularly for males. We conclude that if mortality continues to stagnate at young ages yet declines steadily at old ages, increases in lifespan inequality will become a common feature of future demographic change.},
archivePrefix = {arXiv},
arxivId = {1305.0113},
author = {Gillespie, Duncan O S and Trotter, Meredith V. and Tuljapurkar, Shripad D.},
doi = {10.1007/s13524-014-0287-8},
eprint = {1305.0113},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gillespie, Trotter, Tuljapurkar - 2014 - Divergence in Age Patterns of Mortality Change Drives International Divergence in Lifespan I(2).pdf:pdf},
isbn = {00703370},
issn = {15337790},
journal = {Demography},
keywords = {Disparity,Health,Longevity,Retirement,Social policy},
number = {3},
pages = {1003--1017},
pmid = {24756909},
title = {{Divergence in Age Patterns of Mortality Change Drives International Divergence in Lifespan Inequality}},
volume = {51},
year = {2014}
}
@unpublished{CMI2015a,
address = {London},
author = {{Continuous Mortality Investigation}},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Continuous Mortality Investigation - 2015 - Recent Mortality in England and Wales(2).pdf:pdf},
institution = {Continuous Mortality Investigation Mortality Projections Committee},
series = {CMI Working Paper},
title = {{Recent Mortality in England and Wales}},
year = {2015}
}
@article{Wisniowski2015,
author = {Wi{\'{s}}niowski, Arkadiusz and Smith, Peter W. F. and Bijak, Jakub and Raymer, James and Forster, Jonathan J.},
doi = {10.1007/s13524-015-0389-y},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wi{\'{s}}niowski et al. - 2015 - Bayesian Population Forecasting Extending the Lee-Carter Method(2).pdf:pdf},
issn = {0070-3370},
journal = {Demography},
keywords = {bayesian,lee-carter model,population forecasting,uncertainty},
pages = {1035--1059},
title = {{Bayesian Population Forecasting: Extending the Lee-Carter Method}},
url = {http://link.springer.com/10.1007/s13524-015-0389-y},
volume = {52},
year = {2015}
}
@article{Cairns2007,
abstract = {We compare quantitatively eight stochastic models explaining improvements in mortality rates in England and Wales and in the United States. On the basis of the Bayes Information Criterion (BIC), we find that, for higher ages, an extension of the Cairns-Blake-Dowd (CBD) model that incor- porates a cohort effect fits the England and Wales males data best, while for U.S. males data, the Renshaw and Haberman (RH) extension to the Lee and Carter model that also allows for a cohort effect provides the best fit. However, we identify problems with the robustness of parameter estimates under the RH model, calling into question its suitability for forecasting. A different ex- tension to the CBD model that allows not only for a cohort effect, but also for a quadratic age effect, while ranking below the other models in terms of the BIC, exhibits parameter stability across different time periods for both datasets. This model also shows, for both datasets, that there have been approximately linear improvements over time in mortality rates at all ages, but that the improvements have been greater at lower ages than at higher ages, and that there are significant cohort effects.},
author = {Cairns, Andrew J. G. and Blake, David and Dowd, Kevin and Coughlan, Guy D. and Epstein, David and Ong, Alen and Balevich, Igor},
doi = {10.1080/10920277.2009.10597538},
file = {:C$\backslash$:/mendeleydump/A Quantitative Comparison of Stochastic Mortality Models Using Data From England and Wales and the United States.pdf:pdf},
issn = {1092-0277},
journal = {University Business},
keywords = {Bayes Information Criterion,CBD-Perks models,Lee-Carter models,Stochastic mortality,age e{\textregistered}ect,cohort e{\textregistered}ect,maximum likelihood,parameter stability,period effect,robustness},
number = {March},
pages = {1--35},
title = {{A quantitative comparison of stochastic mortality models using data from England {\&} Wales and the United States}},
volume = {13},
year = {2007}
}
@article{Plat2009,
abstract = {In the last decennium a vast literature on stochastic mortality models has been developed. All well-known models have nice features but also disadvantages. In this paper a stochastic mortality model is proposed that aims at combining the nice features from the existing models, while eliminating the disadvantages. More specifically, the model fits historical data very well, is applicable to a full age range, captures the cohort effect, has a non-trivial (but not too complex) correlation structure and has no robustness problems, while the structure of the model remains relatively simple. Also, the paper describes how to incorporate parameter uncertainty in the model. Furthermore, a risk neutral version of the model is given, that can be used for pricing. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Plat, Richard},
doi = {10.1016/j.insmatheco.2009.08.006},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plat - 2009 - On stochastic mortality modeling.pdf:pdf},
isbn = {9789085707028},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
keywords = {Longevity risk,Monte Carlo simulation,Pricing,Solvency 2,Stochastic mortality models},
number = {3},
pages = {393--404},
publisher = {Elsevier B.V.},
title = {{On stochastic mortality modeling}},
url = {http://dx.doi.org/10.1016/j.insmatheco.2009.08.006},
volume = {45},
year = {2009}
}
@article{Cairns2016,
abstract = {The analysis of national mortality trends is critically dependent on the quality of the population, exposures and deaths data that underpin death rates.We develop a framework that allows us to assess data reliability and to identify anomalies, illustrated, byway of example, using England andWales population data. First, we propose a set of graphical diagnostics that help to pinpoint anomalies. Second, we develop a simple Bayesian model that allows us to quantify objectively the size of any anomalies. Two-dimensional graphical diagnostics and modelling techniques are shown to improve significantly our ability to identify and quantify anomalies. An important conclusion is that significant anomalies in population data can often be linked to uneven patterns of births of people in cohorts born in the distant past. In the case of England and Wales, errors of more than 9{\%} in the estimated size of some birth cohorts can be attributed to an uneven pattern of births.We propose methods that can use births data to improve estimates of the underlying population exposures. Finally, we consider the effect of anomalies on mortality forecasts and annuity values, and we find significant effects for some cohorts. Our methodology has general applicability to other sources of population data, such as the Human Mortality Database.},
author = {Cairns, Andrew J.G. and Blake, David and Dowd, Kevin and Kessler, Amy R.},
doi = {10.1111/rssa.12159},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cairns et al. - 2016 - Phantoms never die living with unreliable population data.pdf:pdf},
issn = {1467985X},
journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
keywords = {Baby boom,Cohortâ€“birthsâ€“deaths exposures methodology,Convexity adjustment ratio,Deaths,Graphical diagnostics,Population data},
number = {4},
pages = {975--1005},
title = {{Phantoms never die: living with unreliable population data}},
volume = {179},
year = {2016}
}
@techreport{Wilmouth2017,
author = {Wilmouth, J R. and Andreev, K. and Jdanov, D. and Glei, D. A. and Riffe, T.},
institution = {Human Mortality Database},
title = {{Methods Protocol for the Human Mortality Database}},
url = {http://www.mortality.org/Public/Docs/MethodsProtocol.pdf},
year = {2017}
}
@techreport{GAD2017,
address = {London},
author = {{Government Actuary's Department}},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Government Actuary's Department - 2017 - Periodic review of rules about State Pension age Report by the Government Actuary(2).pdf:pdf},
isbn = {9781474140294},
title = {{Periodic review of rules about State Pension age Report by the Government Actuary}},
url = {https://www.gov.uk/government/publications/state-pension-age-independent-review-final-report},
year = {2017}
}
@book{Girosi2008,
address = {Princeton},
author = {Girosi, Fredrico and King, Gary},
publisher = {Princeton Univeristy Press},
title = {{Demographic Forecasting}},
year = {2008}
}
@article{Hoffman2014,
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size {\{}$\backslash$epsilon{\}} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter {\{}$\backslash$epsilon{\}} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
archivePrefix = {arXiv},
arxivId = {1111.4246},
author = {Hoffman, M D and Gelman, Andrew},
eprint = {1111.4246},
file = {:C$\backslash$:/Users/jdh1d15/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman, Gelman - 2014 - The no-U-turn sampler Adaptively setting path lengths in Hamiltonian Monte Carlo(2).pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {adaptive monte carlo,bayesian inference,dual averaging,hamiltonian monte carlo,markov chain monte carlo,monte carlo},
number = {2008},
pages = {1--31},
title = {{The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1111.4246},
volume = {15},
year = {2014}
}
